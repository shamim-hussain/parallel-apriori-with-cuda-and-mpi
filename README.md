# parallel_project
Best parallel project in the world!!!

For this project, we will implement the Apriori algorithm for Frequent pattern mining. Frequent pattern mining is a Data Mining problem where we are interested in identifying frequently occurring itemsets in a dataset of multiple transactions.

The support of an itemset in a dataset is defined as the number of transactions in which a given set of items appear as a subset. A frequent pattern is defined as an itemset that has a support greater than a given threshold value (called minimum support or minsup).
The apriori algorithm is a breadth-first (level-wise) approach to finding frequent itemsets which utilizes two facts - if an itemset is frequent, all possible subsets of the itemset must also be frequent and if an itemset is not frequent, any superset of that itemset cannot be frequent. At level l it looks for all frequent patterns of length l. It scans the whole dataset once per level.

At each level, we take four steps. The first step is to generate a candidate itemset based on the frequent itemsets found in the previous level. Then we prune those itemsets that have infrequent subsets. Next, the supports of the valid candidate itemsets are computed by scanning over the dataset. Finally, we remove the candidate itemsets whose support is less than the minimum support and save the rest as frequent itemsets.

The most computationally expensive part of this algorithm is scanning over the whole dataset at each level. This process can be massively parallelized. To implement this in CUDA we propose to use bitset representations of the itemsets, whereby if an item is present its corresponding bit is set to 1, else it is set to 0. The advantage of using this representation is that operations like subset and set intersection can be done with efficient bitwise operations which are very fast on both CPU and GPU. Each thread on the GPU can be assigned to perform subset operation on a transaction of the dataset, allowing for simultaneous scanning of large portions of the datasets. The subset operations will be followed by reduction operations which will add up the number of occurrences and thus compute the support.

On the advent of multiple MPI ranks and GPUs we can partition the dataset and allocate different parts to different MPI ranks. After computing support within each rank, the total support can be calculated by adding up supports found by other ranks - which will require communication among the MPI ranks. If the part of the dataset allocated to each MPI rank is too big to be held in the memory as a whole, new data will be loaded parallelly from disk while the processing is also being carried out. This will require parallel I/O. Also, the found frequent patterns will need to be written back to the disk.
